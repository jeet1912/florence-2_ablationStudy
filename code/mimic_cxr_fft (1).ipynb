{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeWr0AX8EAkd",
        "outputId": "c9713e1f-847e-45d8-fa84-f123c57cf9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2WgFZqFDt21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gcsfs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import io\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from evaluate import load\n",
        "import gc\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qA9QYjsnBX8C"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vTKsuIeQCkkQ"
      },
      "outputs": [],
      "source": [
        "class CXR(Dataset):\n",
        "  def __init__(self, dataframe, processor, max_length):\n",
        "    super().__init__()\n",
        "    self.dataframe = dataframe.reset_index(drop=True)\n",
        "    self.processor = processor\n",
        "    self.max_length = max_length\n",
        "    self.prompt = \"List pathalogical findings for this chest X-ray:\"\n",
        "    self.storage_client = storage.Client(project='silken-physics-467815-g5')\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def _loadImage(self,subject_id, study_id, dicom_id):\n",
        "    try:\n",
        "      bucket_name = \"mimic-cxr-jpg-2.1.0.physionet.org\"\n",
        "      image_path = f\"files/p{subject_id[:2]}/p{subject_id}/s{study_id}/{dicom_id}.jpg\"\n",
        "      bucket = self.storage_client.bucket(bucket_name, user_project='silken-physics-467815-g5')\n",
        "      blob = bucket.blob(image_path)\n",
        "      image_bytes = blob.download_as_bytes()\n",
        "      image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "      return image\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading image {image_path}: {str(e)}\")\n",
        "      return None # Return None if image loading fails\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    row = self.dataframe.iloc[index]\n",
        "    miniReport = str(row['mini_report'])\n",
        "    subject = str(row['subject_id'])\n",
        "    study = str(row['study_id'])\n",
        "    dicom = str(row['dicom_id'])\n",
        "    image = self._loadImage(subject_id=subject,study_id=study,dicom_id=dicom)\n",
        "    inputs = self.processor(images=image, text=self.prompt, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                            truncation=True, max_length=self.max_length)\n",
        "    labels = self.processor.tokenizer(miniReport, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                      truncation=True, max_length=self.max_length)[\"input_ids\"]\n",
        "\n",
        "    return {\n",
        "      \"pixel_values\": inputs[\"pixel_values\"],  # Shape: [1, 3, H, W]\n",
        "      \"input_ids\": inputs[\"input_ids\"],        # Shape: [1, max_length]\n",
        "      \"attention_mask\": inputs[\"attention_mask\"],  # Shape: [1, max_length]\n",
        "      \"labels\": labels                         # Shape: [1, max_length]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "8SB0At4CCobu",
        "outputId": "cba21b47-20a4-4a44-f6e5-5d8b702e4344"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c966a0cd76c844ac81f13c277a295332",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "713b1c2cf98b449c951f1154156f16c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f564173268ab45cf9144bd0a88eb64a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "daa5324403704be391cff4473f322a8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75c91532f9ac4081ae2508ca2fc119e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9lx1KR2Ctz9",
        "outputId": "6bdafc18-bed1-4bfa-8181-3b6dc0720212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of mini-reports: 92\n"
          ]
        }
      ],
      "source": [
        "train_df =pd.read_csv('./train_split.csv')\n",
        "test_df =pd.read_csv('./test_split.csv')\n",
        "val_df=pd.read_csv('./val_split.csv')\n",
        "max_length = max(len(processor.tokenizer.encode(report)) for report in train_df['mini_report'])\n",
        "print(f\"Max length of mini-reports: {max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mVZPVic0Curd"
      },
      "outputs": [],
      "source": [
        "train_dataset = CXR(train_df, processor, max_length)\n",
        "val_dataset = CXR(val_df, processor, max_length)\n",
        "test_dataset = CXR(test_df, processor, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ega_aP4Cw1C",
        "outputId": "18f9dd92-3e93-4a34-af9e-94740f82f9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0]['pixel_values'].shape)\n",
        "print(train_dataset[0]['input_ids'].shape)\n",
        "print(train_dataset[0]['attention_mask'].shape)\n",
        "print(train_dataset[0]['labels'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQliT8RyC1YT"
      },
      "source": [
        "## Load GIT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "tCKWDMc4CzFm",
        "outputId": "8024655a-0c13-4ad4-fd3c-06cd58f92312"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6775a8b15a5d41d18eb3c63ace9d18ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cb76ccef5164854803f664675277346",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/707M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fb31c7aeeff4399b1a8faf2e7615c13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7vg3DIBC66P",
        "outputId": "e1cddf4a-304c-4300-efa3-4de55a7ebb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory Allocated before loading GIT Large : 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "print('Memory Allocated before loading GIT Large :',memory_allocated,'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlj4A9NcC9RU",
        "outputId": "118db7eb-de0a-43a9-bed9-37311199737f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GitForCausalLM(\n",
              "  (git): GitModel(\n",
              "    (embeddings): GitEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(1024, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (image_encoder): GitVisionModel(\n",
              "      (vision_model): GitVisionTransformer(\n",
              "        (embeddings): GitVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "          (position_embedding): Embedding(197, 768)\n",
              "        )\n",
              "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder): GitVisionEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-11): 12 x GitVisionEncoderLayer(\n",
              "              (self_attn): GitVisionAttention(\n",
              "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): GitVisionMLP(\n",
              "                (activation_fn): QuickGELUActivation()\n",
              "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (encoder): GitEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x GitLayer(\n",
              "          (attention): GitAttention(\n",
              "            (self): GitSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): GitSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): GitIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): GitOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (visual_projection): GitProjection(\n",
              "      (visual_projection): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2_yvHk2DFHH",
        "outputId": "9af4d5f2-465b-4931-b415-716949b623d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory Allocated after loading GIT Base : 674.91845703125 MB\n"
          ]
        }
      ],
      "source": [
        "memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "print('Memory Allocated after loading GIT Base :',memory_allocated,'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "0bAsi8TvDF94",
        "outputId": "32176a03-8a8a-48ba-aeac-cd0976a6dfcd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3101329110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuBsk2iDWx0"
      },
      "source": [
        "## Dataloader helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Gxs5V4bgDF8U"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch], dim=0)  # [batch_size, 3, H, W]\n",
        "    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)        # [batch_size, max_length]\n",
        "    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)  # [batch_size, max_length]\n",
        "    labels = torch.cat([item[\"labels\"] for item in batch], dim=0)               # [batch_size, max_length]\n",
        "\n",
        "    #print(f\"Batch shapes: pixel_values={pixel_values.shape}, input_ids={input_ids.shape}, \"\n",
        "    #      f\"attention_mask={attention_mask.shape}, labels={labels.shape}\")\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z5f_wJBDb6a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rWfeuiepDZ1x"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkFbQWAD5zWx",
        "outputId": "31522d80-1a4e-4b87-8938-4143cde52708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 RESOURCE-MANAGED TRAINING SETUP\n",
            "=====================================\n",
            "\n",
            "BEFORE RUNNING:\n",
            "1. Define your train_loader and val_loader\n",
            "2. Ensure your dataset and collate_fn are ready\n",
            "3. Update CHECKPOINT_DIR and RESULTS_DIR paths if needed\n",
            "\n",
            "TO RUN:\n",
            "run_resource_managed_training()\n",
            "\n",
            "WORKFLOW:\n",
            "- Trains 4 epochs per session\n",
            "- Saves checkpoints automatically  \n",
            "- Disconnects → waits → reconnects for fresh GPU\n",
            "- Resumes automatically from latest checkpoint\n",
            "- Saves final results when complete\n",
            "\n",
            "BENEFITS:\n",
            "✅ Avoids degraded GPU instances\n",
            "✅ Saves compute units with efficient training\n",
            "✅ Automatic resume from failures\n",
            "✅ Memory management built-in\n",
            "✅ Performance monitoring\n",
            "\n",
            "🧪 Testing GPU performance...\n",
            "GPU benchmark: 0.06s\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "✅ GPU performance looks good!\n",
            "🔍 Found latest checkpoint: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_18.pt\n",
            "🔄 Resuming from checkpoint...\n",
            "📂 Loading checkpoint: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_18.pt\n",
            "📍 Resuming from epoch 18\n",
            "================================================================================\n",
            "🎯 Training Configuration:\n",
            "   Batch Size: 64\n",
            "   Learning Rate: 1e-05\n",
            "   Total Epochs: 20\n",
            "   Epochs per Chunk: 4\n",
            "   Trainable Parameters: 176,619,066\n",
            "================================================================================\n",
            "📊 This session: epochs 19 to 20\n",
            "💾 Results will be saved to: /content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results\n",
            "🚀 Training epochs 19 to 20\n",
            "\n",
            "--- Epoch 19/20 ---\n",
            "  Batch 1 completed, loss: 11.030179\n",
            "  ✅ Epoch 19 completed in 356.6s\n",
            "     Train Loss: 11.030179\n",
            "     Val Loss: 11.071933\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 8106.7s\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_19.pt\n",
            "\n",
            "--- Epoch 20/20 ---\n",
            "  Batch 1 completed, loss: 11.058867\n",
            "  ✅ Epoch 20 completed in 398.7s\n",
            "     Train Loss: 11.058867\n",
            "     Val Loss: 11.040132\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 8506.9s\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_20.pt\n",
            "\n",
            "✅ Chunk completed successfully!\n",
            "   Epochs completed: 20/20\n",
            "🎉 Training completed! Saving final results...\n",
            "✅ Best weights loaded from /content/drive/MyDrive/GIT_checkpoints/best_weights.pt\n",
            "💾 Final results saved to: /content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results/ffttrain_target_modules_ablation_results.csv\n",
            "⏱️  Total training time: 8509.2 seconds\n",
            "🧹 Memory cleaned up\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForCausalLM\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# =============================================================================\n",
        "# RESOURCE MANAGEMENT CONFIGURATION\n",
        "# =============================================================================\n",
        "EPOCHS_PER_CHUNK = 4  # Train 2 epochs per session, then restart\n",
        "TOTAL_EPOCHS = 20\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/GIT_checkpoints/'\n",
        "RESULTS_DIR = '/content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# CHECKPOINT MANAGEMENT FUNCTIONS\n",
        "# =============================================================================\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, train_losses, val_losses, memory_usage,\n",
        "                   results, start_time, best_val_loss, epochs_with_no_improvement, best_weights=None):\n",
        "    \"\"\"Save complete training state\"\"\"\n",
        "\n",
        "    # Save or reference best weights\n",
        "    best_weights_path = os.path.join(CHECKPOINT_DIR, \"best_weights.pt\")\n",
        "    if best_weights is not None:\n",
        "        torch.save(best_weights, best_weights_path)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'memory_usage': memory_usage,\n",
        "        'results': results,\n",
        "        'start_time': start_time,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_with_no_improvement': epochs_with_no_improvement,\n",
        "        'best_weights_path': best_weights_path\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pt')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"✅ Checkpoint saved: {checkpoint_path}\")\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    \"\"\"Load complete training state\"\"\"\n",
        "    print(f\"📂 Loading checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def find_latest_checkpoint():\n",
        "    \"\"\"Find the most recent checkpoint\"\"\"\n",
        "    checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('checkpoint_epoch_')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "\n",
        "    # Sort by epoch number\n",
        "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest = os.path.join(CHECKPOINT_DIR, checkpoints[-1])\n",
        "    print(f\"🔍 Found latest checkpoint: {latest}\")\n",
        "    return latest\n",
        "\n",
        "# =============================================================================\n",
        "# GPU PERFORMANCE CHECK\n",
        "# =============================================================================\n",
        "def quick_gpu_benchmark():\n",
        "    \"\"\"Quick GPU performance test\"\"\"\n",
        "    print(\"🧪 Testing GPU performance...\")\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"❌ CUDA not available!\")\n",
        "        return False\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    a = torch.randn(2000, 2000, device=device)\n",
        "    b = torch.randn(2000, 2000, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(50):\n",
        "        c = torch.mm(a, b)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    benchmark_time = time.time() - start_time\n",
        "\n",
        "    print(f\"GPU benchmark: {benchmark_time:.2f}s\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "    # Clean up\n",
        "    del a, b, c\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if benchmark_time > 3.0:  # Should be ~1-2s on healthy A100\n",
        "        print(\"⚠️  WARNING: GPU performance seems degraded!\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ GPU performance looks good!\")\n",
        "        return True\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED TRAINING FUNCTIONS\n",
        "# =============================================================================\n",
        "def make_train_step_with_monitoring(model):\n",
        "    def train_step(batch):\n",
        "        model.train()\n",
        "        batch_start = time.time()\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        if batch_time > 10.0:  # Warn if batch takes >10s\n",
        "            print(f\"⚠️  Slow batch: {batch_time:.2f}s\")\n",
        "\n",
        "        return loss.item()\n",
        "    return train_step\n",
        "\n",
        "def make_val_step_with_monitoring(model):\n",
        "    def val_step(batch):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            return loss.item()\n",
        "    return val_step\n",
        "\n",
        "def train_chunk(start_epoch, end_epoch, model, train_loader, val_loader, optimizer,\n",
        "                scheduler, train_losses, val_losses, memory_usage, best_val_loss,\n",
        "                epochs_with_no_improvement, start_time):\n",
        "    \"\"\"Train for a chunk of epochs with monitoring\"\"\"\n",
        "\n",
        "    train_step = make_train_step_with_monitoring(model)\n",
        "    val_step = make_val_step_with_monitoring(model)\n",
        "\n",
        "    print(f\"🚀 Training epochs {start_epoch+1} to {end_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        epoch_start = time.time()\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{TOTAL_EPOCHS} ---\")\n",
        "\n",
        "        # Training\n",
        "        train_loss = 0.0\n",
        "        train_samples = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            if batch_idx == 0:  # Only process first batch for testing\n",
        "                loss = train_step(batch)\n",
        "                batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "                train_loss += loss * batch_size_actual\n",
        "                train_samples += batch_size_actual\n",
        "                batch_count += 1\n",
        "                print(f\"  Batch {batch_idx+1} completed, loss: {loss:.6f}\")\n",
        "                break\n",
        "\n",
        "        avg_train_loss = train_loss / train_samples if train_samples > 0 else 0\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                loss = val_step(batch)\n",
        "                batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "                val_loss += loss * batch_size_actual\n",
        "                val_samples += batch_size_actual\n",
        "                if batch_idx >= 4:  # Limit validation batches for speed\n",
        "                    break\n",
        "\n",
        "        avg_val_loss = val_loss / val_samples if val_samples > 0 else 0\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Memory tracking\n",
        "        torch.cuda.synchronize()\n",
        "        memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "        memory_usage.append(memory_mb)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(f\"  ✅ Epoch {epoch+1} completed in {epoch_time:.1f}s\")\n",
        "        print(f\"     Train Loss: {avg_train_loss:.6f}\")\n",
        "        print(f\"     Val Loss: {avg_val_loss:.6f}\")\n",
        "        print(f\"     Memory: {memory_mb:.0f} MB\")\n",
        "        print(f\"     Total Time: {total_time:.1f}s\")\n",
        "\n",
        "        # Scheduler and early stopping\n",
        "        scheduler.step(avg_val_loss)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_weights = model.state_dict()  # Store best weights\n",
        "            torch.save(best_weights, os.path.join(CHECKPOINT_DIR, \"best_weights.pt\"))\n",
        "            epochs_with_no_improvement = 0\n",
        "            print(f\"     🎯 New best validation loss: {best_val_loss:.6f}\")\n",
        "        else:\n",
        "            epochs_with_no_improvement += 1\n",
        "            best_weights = None  # No improvement this epoch\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        save_checkpoint(model, optimizer, scheduler, epoch + 1, train_losses,\n",
        "                       val_losses, memory_usage, [], start_time, best_val_loss,\n",
        "                       epochs_with_no_improvement, best_weights)\n",
        "\n",
        "    return train_losses, val_losses, memory_usage, best_val_loss, epochs_with_no_improvement\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN TRAINING ORCHESTRATOR\n",
        "# =============================================================================\n",
        "def run_resource_managed_training():\n",
        "    \"\"\"Main training function with resource management\"\"\"\n",
        "\n",
        "    # Check GPU performance first\n",
        "    if not quick_gpu_benchmark():\n",
        "        print(\"🔄 Consider restarting runtime for better GPU performance\")\n",
        "        return\n",
        "\n",
        "    # Initialize training parameters\n",
        "    batch_size = 64\n",
        "    learning_rate = 1e-5\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Check for existing checkpoint\n",
        "    latest_checkpoint = find_latest_checkpoint()\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        print(\"🔄 Resuming from checkpoint...\")\n",
        "        # Load model architecture first\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "        model.to(device)\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = load_checkpoint(latest_checkpoint, model, optimizer, scheduler)\n",
        "\n",
        "        # Restore training state\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        val_losses = checkpoint['val_losses']\n",
        "        memory_usage = checkpoint['memory_usage']\n",
        "        start_time = checkpoint['start_time']\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        epochs_with_no_improvement = checkpoint['epochs_with_no_improvement']\n",
        "\n",
        "        print(f\"📍 Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    else:\n",
        "        print(\"🆕 Starting fresh training...\")\n",
        "        # Initialize everything from scratch\n",
        "        start_time = time.time()\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "        start_epoch = 0\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        memory_usage = []\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_with_no_improvement = 0\n",
        "\n",
        "    print('='*80)\n",
        "    print(f'🎯 Training Configuration:')\n",
        "    print(f'   Batch Size: {batch_size}')\n",
        "    print(f'   Learning Rate: {learning_rate}')\n",
        "    print(f'   Total Epochs: {TOTAL_EPOCHS}')\n",
        "    print(f'   Epochs per Chunk: {EPOCHS_PER_CHUNK}')\n",
        "    print(f'   Trainable Parameters: {model.num_parameters():,}')\n",
        "    print('='*80)\n",
        "\n",
        "    # DEFINE YOUR DATALOADERS HERE\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Calculate remaining epochs and chunks\n",
        "    remaining_epochs = TOTAL_EPOCHS - start_epoch\n",
        "    if remaining_epochs <= 0:\n",
        "        print(\"✅ Training already completed!\")\n",
        "        return\n",
        "\n",
        "    # Determine chunk size for this session\n",
        "    epochs_this_session = min(EPOCHS_PER_CHUNK, remaining_epochs)\n",
        "    end_epoch = start_epoch + epochs_this_session\n",
        "\n",
        "    print(f\"📊 This session: epochs {start_epoch+1} to {end_epoch}\")\n",
        "    print(f\"💾 Results will be saved to: {RESULTS_DIR}\")\n",
        "\n",
        "    # Train the chunk\n",
        "    try:\n",
        "        train_losses, val_losses, memory_usage, best_val_loss, epochs_with_no_improvement = train_chunk(\n",
        "            start_epoch, end_epoch, model, train_loader, val_loader, optimizer,\n",
        "            scheduler, train_losses, val_losses, memory_usage, best_val_loss,\n",
        "            epochs_with_no_improvement, start_time\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Chunk completed successfully!\")\n",
        "        print(f\"   Epochs completed: {end_epoch}/{TOTAL_EPOCHS}\")\n",
        "\n",
        "        # If training complete, save final results\n",
        "        if end_epoch >= TOTAL_EPOCHS:\n",
        "            print(\"🎉 Training completed! Saving final results...\")\n",
        "\n",
        "            # Load best weights\n",
        "            best_weights_path = os.path.join(CHECKPOINT_DIR, \"best_weights.pt\")\n",
        "            if os.path.exists(best_weights_path):\n",
        "                best_weights = torch.load(best_weights_path)\n",
        "                print(f\"✅ Best weights loaded from {best_weights_path}\")\n",
        "            else:\n",
        "                best_weights = model.state_dict()\n",
        "                print(\"⚠️  No best weights file found, using final model state\")\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            results = [{\n",
        "                \"experiment\": \"fft\",\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_losses\": val_losses,\n",
        "                \"memory_usage_MB\": memory_usage,\n",
        "                \"training_time_seconds\": training_time,\n",
        "                \"best_val_loss\": best_val_loss,\n",
        "                \"best_weights\": best_weights  # Added missing best_weights\n",
        "            }]\n",
        "\n",
        "            # Save results\n",
        "            results_df = pd.DataFrame(results)\n",
        "            results_path = os.path.join(RESULTS_DIR, 'ffttrain_target_modules_ablation_results.csv')\n",
        "            results_df.to_csv(results_path, index=False)\n",
        "\n",
        "            print(f\"💾 Final results saved to: {results_path}\")\n",
        "            print(f\"⏱️  Total training time: {training_time:.1f} seconds\")\n",
        "\n",
        "        else:\n",
        "            print(f\"\\n⏭️  Next steps:\")\n",
        "            print(f\"   1. Runtime → Disconnect and delete runtime\")\n",
        "            print(f\"   2. Wait 2-3 minutes\")\n",
        "            print(f\"   3. Reconnect and run this code again\")\n",
        "            print(f\"   4. Training will resume from epoch {end_epoch+1}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during training: {e}\")\n",
        "        print(\"💾 Checkpoint should be saved. You can resume later.\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"🧹 Memory cleaned up\")\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE INSTRUCTIONS\n",
        "# =============================================================================\n",
        "print(\"\"\"\n",
        "🚀 RESOURCE-MANAGED TRAINING SETUP\n",
        "=====================================\n",
        "\n",
        "BEFORE RUNNING:\n",
        "1. Define your train_loader and val_loader\n",
        "2. Ensure your dataset and collate_fn are ready\n",
        "3. Update CHECKPOINT_DIR and RESULTS_DIR paths if needed\n",
        "\n",
        "TO RUN:\n",
        "run_resource_managed_training()\n",
        "\n",
        "WORKFLOW:\n",
        "- Trains 4 epochs per session\n",
        "- Saves checkpoints automatically\n",
        "- Disconnects → waits → reconnects for fresh GPU\n",
        "- Resumes automatically from latest checkpoint\n",
        "- Saves final results when complete\n",
        "\n",
        "BENEFITS:\n",
        "✅ Avoids degraded GPU instances\n",
        "✅ Saves compute units with efficient training\n",
        "✅ Automatic resume from failures\n",
        "✅ Memory management built-in\n",
        "✅ Performance monitoring\n",
        "\"\"\")\n",
        "\n",
        "# Uncomment to run (after adding your dataloaders):\n",
        "run_resource_managed_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpuhqNKXDnIZ"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f0m7_YrDkUT",
        "outputId": "edcdf1f6-7d40-4ffa-eb47-428ea0824ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results with Best Model:\n",
            "Test Loss: 11.193929, BLEU Score: 0.0243\n",
            "\n",
            "FFT Ablation Study Testing Results (Target Modules):\n",
            "   experiment   test_loss                                         bleu_score\n",
            "0  fftTesting  716.411438  {'bleu': 0.024314319549676687, 'precisions': [...\n"
          ]
        }
      ],
      "source": [
        "# Fix 1: Set tokenizer padding to left side for decoder-only models\n",
        "processor.tokenizer.padding_side = 'left'\n",
        "# Load and prepare model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/GIT_checkpoints/best_weights.pt', weights_only=True))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loss = 0.0\n",
        "test_samples = 0\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "results = []\n",
        "bleu = load(\"bleu\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "        test_loss += loss.item() * batch_size_actual\n",
        "        test_samples += batch_size_actual\n",
        "\n",
        "        # Generate predictions\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "\n",
        "        # Fix 2: Use max_new_tokens instead of max_length to avoid length conflicts\n",
        "        generated_ids = model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=92,  # Generate up to 50 new tokens\n",
        "            do_sample=False,    # Use greedy decoding for reproducible results\n",
        "            pad_token_id=processor.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        predictions.extend([text.replace(\"List pathalogical findings for this chest X-ray:\", \"\").strip() for text in generated_texts])\n",
        "\n",
        "        # Decode labels for references\n",
        "        label_ids = batch[\"labels\"]\n",
        "        reference_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        references.extend(reference_texts)\n",
        "        break\n",
        "\n",
        "avg_test_loss = test_loss / test_samples\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "print(f\"Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {avg_test_loss:.6f}, BLEU Score: {bleu_score['bleu']:.4f}\")\n",
        "results.append({\n",
        "        \"experiment\": \"fftTesting\",\n",
        "        \"test_loss\": test_loss,\n",
        "        \"bleu_score\": bleu_score\n",
        "    })\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "folder_path = '/content/drive/MyDrive/GIT AblationStratergy/fft_test_ablationStudy/results'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(os.path.join(folder_path, 'fftTest_target_modules_ablation_results.csv'), index=False)\n",
        "print(\"\\nFFT Ablation Study Testing Results (Target Modules):\")\n",
        "print(results_df[[\"experiment\", \"test_loss\", \"bleu_score\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImxUv-P-GfDX",
        "outputId": "126c8757-ae84-4fd6-9dd0-931e3f63dc81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
