{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeWr0AX8EAkd",
        "outputId": "c9713e1f-847e-45d8-fa84-f123c57cf9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D2WgFZqFDt21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gcsfs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import io\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from evaluate import load\n",
        "import gc\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qA9QYjsnBX8C"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vTKsuIeQCkkQ"
      },
      "outputs": [],
      "source": [
        "class CXR(Dataset):\n",
        "  def __init__(self, dataframe, processor, max_length):\n",
        "    super().__init__()\n",
        "    self.dataframe = dataframe.reset_index(drop=True)\n",
        "    self.processor = processor\n",
        "    self.max_length = max_length\n",
        "    self.prompt = \"List pathalogical findings for this chest X-ray:\"\n",
        "    self.storage_client = storage.Client(project='silken-physics-467815-g5')\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def _loadImage(self,subject_id, study_id, dicom_id):\n",
        "    try:\n",
        "      bucket_name = \"mimic-cxr-jpg-2.1.0.physionet.org\"\n",
        "      image_path = f\"files/p{subject_id[:2]}/p{subject_id}/s{study_id}/{dicom_id}.jpg\"\n",
        "      bucket = self.storage_client.bucket(bucket_name, user_project='silken-physics-467815-g5')\n",
        "      blob = bucket.blob(image_path)\n",
        "      image_bytes = blob.download_as_bytes()\n",
        "      image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "      return image\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading image {image_path}: {str(e)}\")\n",
        "      return None # Return None if image loading fails\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    row = self.dataframe.iloc[index]\n",
        "    miniReport = str(row['mini_report'])\n",
        "    subject = str(row['subject_id'])\n",
        "    study = str(row['study_id'])\n",
        "    dicom = str(row['dicom_id'])\n",
        "    image = self._loadImage(subject_id=subject,study_id=study,dicom_id=dicom)\n",
        "    inputs = self.processor(images=image, text=self.prompt, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                            truncation=True, max_length=self.max_length)\n",
        "    labels = self.processor.tokenizer(miniReport, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                      truncation=True, max_length=self.max_length)[\"input_ids\"]\n",
        "\n",
        "    return {\n",
        "      \"pixel_values\": inputs[\"pixel_values\"],  # Shape: [1, 3, H, W]\n",
        "      \"input_ids\": inputs[\"input_ids\"],        # Shape: [1, max_length]\n",
        "      \"attention_mask\": inputs[\"attention_mask\"],  # Shape: [1, max_length]\n",
        "      \"labels\": labels                         # Shape: [1, max_length]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9lx1KR2Ctz9",
        "outputId": "6bdafc18-bed1-4bfa-8181-3b6dc0720212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of mini-reports: 92\n"
          ]
        }
      ],
      "source": [
        "train_df =pd.read_csv('./train_split.csv')\n",
        "test_df =pd.read_csv('./test_split.csv')\n",
        "val_df=pd.read_csv('./val_split.csv')\n",
        "max_length = max(len(processor.tokenizer.encode(report)) for report in train_df['mini_report'])\n",
        "print(f\"Max length of mini-reports: {max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mVZPVic0Curd"
      },
      "outputs": [],
      "source": [
        "train_dataset = CXR(train_df, processor, max_length)\n",
        "val_dataset = CXR(val_df, processor, max_length)\n",
        "test_dataset = CXR(test_df, processor, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ega_aP4Cw1C",
        "outputId": "18f9dd92-3e93-4a34-af9e-94740f82f9ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0]['pixel_values'].shape)\n",
        "print(train_dataset[0]['input_ids'].shape)\n",
        "print(train_dataset[0]['attention_mask'].shape)\n",
        "print(train_dataset[0]['labels'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQliT8RyC1YT"
      },
      "source": [
        "## Load GIT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "tCKWDMc4CzFm",
        "outputId": "8024655a-0c13-4ad4-fd3c-06cd58f92312"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\", trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7vg3DIBC66P",
        "outputId": "e1cddf4a-304c-4300-efa3-4de55a7ebb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory Allocated before loading GIT Large : 0.0 MB\n"
          ]
        }
      ],
      "source": [
        "memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "print('Memory Allocated before loading GIT Large :',memory_allocated,'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlj4A9NcC9RU",
        "outputId": "118db7eb-de0a-43a9-bed9-37311199737f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GitForCausalLM(\n",
              "  (git): GitModel(\n",
              "    (embeddings): GitEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(1024, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (image_encoder): GitVisionModel(\n",
              "      (vision_model): GitVisionTransformer(\n",
              "        (embeddings): GitVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
              "          (position_embedding): Embedding(197, 768)\n",
              "        )\n",
              "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder): GitVisionEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-11): 12 x GitVisionEncoderLayer(\n",
              "              (self_attn): GitVisionAttention(\n",
              "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              )\n",
              "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): GitVisionMLP(\n",
              "                (activation_fn): QuickGELUActivation()\n",
              "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (encoder): GitEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x GitLayer(\n",
              "          (attention): GitAttention(\n",
              "            (self): GitSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): GitSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): GitIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): GitOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (visual_projection): GitProjection(\n",
              "      (visual_projection): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2_yvHk2DFHH",
        "outputId": "9af4d5f2-465b-4931-b415-716949b623d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory Allocated after loading GIT Base : 674.91845703125 MB\n"
          ]
        }
      ],
      "source": [
        "memory_allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "print('Memory Allocated after loading GIT Base :',memory_allocated,'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "0bAsi8TvDF94",
        "outputId": "32176a03-8a8a-48ba-aeac-cd0976a6dfcd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuBsk2iDWx0"
      },
      "source": [
        "## Dataloader helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Gxs5V4bgDF8U"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch], dim=0)  # [batch_size, 3, H, W]\n",
        "    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)        # [batch_size, max_length]\n",
        "    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)  # [batch_size, max_length]\n",
        "    labels = torch.cat([item[\"labels\"] for item in batch], dim=0)               # [batch_size, max_length]\n",
        "\n",
        "    #print(f\"Batch shapes: pixel_values={pixel_values.shape}, input_ids={input_ids.shape}, \"\n",
        "    #      f\"attention_mask={attention_mask.shape}, labels={labels.shape}\")\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z5f_wJBDb6a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rWfeuiepDZ1x"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "fwCvN2gyDhei",
        "outputId": "42b83be9-9624-4884-8d44-8ca20c61ec4a"
      },
      "outputs": [],
      "source": [
        "# batch_size = 64\n",
        "# learning_rate = 1e-5\n",
        "# start_time = time.time()  # Record start time\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "# # Initialize model\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "# model.to(device)\n",
        "# print('------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "# print(f'Hyperparameters are Batch Size = {batch_size}, Learning Rate = {learning_rate} with {model.num_parameters()} trainable parameters')\n",
        "# print('------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "# # Create dataloaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# results = []\n",
        "# def make_train_step(model):\n",
        "#     def train_step(batch):\n",
        "#         model.train()\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#         outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "#         return loss.item()\n",
        "#     return train_step\n",
        "\n",
        "# def make_val_step(model):\n",
        "#     def val_step(batch):\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#             outputs = model(**batch)\n",
        "#             loss = outputs.loss\n",
        "#             return loss.item()\n",
        "#     return val_step\n",
        "\n",
        "# def for_epochs(epochs, train_loader, val_loader, train_step, val_step, scheduler, early_stop_window=2):\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     memory_usage = []\n",
        "#     best_val_loss = float('inf')\n",
        "#     best_weights = None\n",
        "#     epochs_with_no_improvement = 0\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         # Training\n",
        "#         train_loss = 0.0\n",
        "#         train_samples = 0\n",
        "#         for batch in train_loader:\n",
        "#             loss = train_step(batch)\n",
        "#             batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "#             train_loss += loss * batch_size_actual\n",
        "#             train_samples += batch_size_actual\n",
        "#             break\n",
        "#         avg_train_loss = train_loss / train_samples\n",
        "#         train_losses.append(avg_train_loss)\n",
        "\n",
        "\n",
        "#         # Validation\n",
        "#         val_loss = 0.0\n",
        "#         val_samples = 0\n",
        "#         with torch.no_grad():\n",
        "#             for batch in val_loader:\n",
        "#                 loss = val_step(batch)\n",
        "#                 batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "#                 val_loss += loss * batch_size_actual\n",
        "#                 val_samples += batch_size_actual\n",
        "#         avg_val_loss = val_loss / val_samples\n",
        "#         val_losses.append(avg_val_loss)\n",
        "\n",
        "#         # Memory tracking\n",
        "#         torch.cuda.synchronize()\n",
        "#         memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
        "#         memory_usage.append(memory_mb)\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, Memory: {memory_mb:.2f} MB\")\n",
        "\n",
        "#         # Scheduler and early stopping\n",
        "#         scheduler.step(avg_val_loss)\n",
        "#         if avg_val_loss < best_val_loss:\n",
        "#             best_val_loss = avg_val_loss\n",
        "#             torch.save(model.state_dict(), \"best_weights.pt\")\n",
        "#             epochs_with_no_improvement = 0\n",
        "#         else:\n",
        "#             epochs_with_no_improvement += 1\n",
        "#             if epochs_with_no_improvement >= early_stop_window:\n",
        "#                 print(\"Early stopping triggered\")\n",
        "#     best_weights = torch.load(\"best_weights.pt\")\n",
        "\n",
        "#     training_time = time.time() - start_time\n",
        "#     print(f\"Total Training Time: {training_time:.4f} seconds\")\n",
        "#     return train_losses, val_losses, memory_usage, best_weights, training_time\n",
        "\n",
        "# train_step = make_train_step(model)\n",
        "# val_step = make_val_step(model)\n",
        "# train_losses, val_losses, memory_usage, best_weights, training_time = for_epochs(5, train_loader, val_loader, train_step, val_step, scheduler)\n",
        "# results.append({\n",
        "#         \"experiment\": \"fft\",\n",
        "#         \"train_losses\": train_losses,\n",
        "#         \"val_losses\": val_losses,\n",
        "#         \"memory_usage_MB\": memory_usage,\n",
        "#         \"training_time_seconds\": training_time\n",
        "#     })\n",
        "# del model\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "\n",
        "# folder_path = '/content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results'\n",
        "# os.makedirs(folder_path, exist_ok=True)\n",
        "# results_df = pd.DataFrame(results)\n",
        "# results_df.to_csv(os.path.join(folder_path, 'ffttrain_target_modules_ablation_results.csv'), index=False)\n",
        "# print(\"\\n Fft Ablation Study Training Results (Target Modules):\")\n",
        "# print(results_df[[\"experiment\", \"test_loss\", \"bleu_score\", \"training_time_seconds\", \"trainable_parameters\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "rF0hwdlUER9p",
        "outputId": "0db93436-ab8c-40af-c777-0c11e7d3c32c"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkFbQWAD5zWx",
        "outputId": "fb7cc2f4-357e-4679-ee8e-db9d6c3cb827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 RESOURCE-MANAGED TRAINING SETUP\n",
            "=====================================\n",
            "\n",
            "BEFORE RUNNING:\n",
            "1. Define your train_loader and val_loader\n",
            "2. Ensure your dataset and collate_fn are ready\n",
            "3. Update CHECKPOINT_DIR and RESULTS_DIR paths if needed\n",
            "\n",
            "TO RUN:\n",
            "run_resource_managed_training()\n",
            "\n",
            "WORKFLOW:\n",
            "- Trains 4 epochs per session\n",
            "- Saves checkpoints automatically  \n",
            "- Disconnects → waits → reconnects for fresh GPU\n",
            "- Resumes automatically from latest checkpoint\n",
            "- Saves final results when complete\n",
            "\n",
            "BENEFITS:\n",
            "✅ Avoids degraded GPU instances\n",
            "✅ Saves compute units with efficient training\n",
            "✅ Automatic resume from failures\n",
            "✅ Memory management built-in\n",
            "✅ Performance monitoring\n",
            "\n",
            "🧪 Testing GPU performance...\n",
            "GPU benchmark: 0.06s\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "✅ GPU performance looks good!\n",
            "🔍 Found latest checkpoint: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_6.pt\n",
            "🔄 Resuming from checkpoint...\n",
            "📂 Loading checkpoint: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_6.pt\n",
            "📍 Resuming from epoch 6\n",
            "================================================================================\n",
            "🎯 Training Configuration:\n",
            "   Batch Size: 64\n",
            "   Learning Rate: 1e-05\n",
            "   Total Epochs: 20\n",
            "   Epochs per Chunk: 4\n",
            "   Trainable Parameters: 176,619,066\n",
            "================================================================================\n",
            "📊 This session: epochs 7 to 10\n",
            "💾 Results will be saved to: /content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results\n",
            "🚀 Training epochs 7 to 10\n",
            "\n",
            "--- Epoch 7/20 ---\n",
            "  Batch 1 completed, loss: 10.916970\n",
            "  ✅ Epoch 7 completed in 360.7s\n",
            "     Train Loss: 10.916970\n",
            "     Val Loss: 11.093723\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 2786.4s\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_7.pt\n",
            "\n",
            "--- Epoch 8/20 ---\n",
            "  Batch 1 completed, loss: 11.016291\n",
            "  ✅ Epoch 8 completed in 404.9s\n",
            "     Train Loss: 11.016291\n",
            "     Val Loss: 11.053852\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 3192.8s\n",
            "     🎯 New best validation loss: 11.053852\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_8.pt\n",
            "\n",
            "--- Epoch 9/20 ---\n",
            "  Batch 1 completed, loss: 10.866901\n",
            "  ✅ Epoch 9 completed in 359.2s\n",
            "     Train Loss: 10.866901\n",
            "     Val Loss: 11.105085\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 3557.1s\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_9.pt\n",
            "\n",
            "--- Epoch 10/20 ---\n",
            "  Batch 1 completed, loss: 11.125257\n",
            "  ✅ Epoch 10 completed in 372.8s\n",
            "     Train Loss: 11.125257\n",
            "     Val Loss: 11.084622\n",
            "     Memory: 4752 MB\n",
            "     Total Time: 3931.4s\n",
            "✅ Checkpoint saved: /content/drive/MyDrive/GIT_checkpoints/checkpoint_epoch_10.pt\n",
            "\n",
            "✅ Chunk completed successfully!\n",
            "   Epochs completed: 10/20\n",
            "\n",
            "⏭️  Next steps:\n",
            "   1. Runtime → Disconnect and delete runtime\n",
            "   2. Wait 2-3 minutes\n",
            "   3. Reconnect and run this code again\n",
            "   4. Training will resume from epoch 11\n",
            "🧹 Memory cleaned up\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForCausalLM\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# =============================================================================\n",
        "# RESOURCE MANAGEMENT CONFIGURATION\n",
        "# =============================================================================\n",
        "EPOCHS_PER_CHUNK = 4  # Train 2 epochs per session, then restart\n",
        "TOTAL_EPOCHS = 20\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/GIT_checkpoints/'\n",
        "RESULTS_DIR = '/content/drive/MyDrive/GIT AblationStratergy/fft_ablationStudy/results'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# CHECKPOINT MANAGEMENT FUNCTIONS\n",
        "# =============================================================================\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, train_losses, val_losses, memory_usage,\n",
        "                   results, start_time, best_val_loss, epochs_with_no_improvement, best_weights=None):\n",
        "    \"\"\"Save complete training state\"\"\"\n",
        "\n",
        "    # Save or reference best weights\n",
        "    best_weights_path = os.path.join(CHECKPOINT_DIR, \"best_weights.pt\")\n",
        "    if best_weights is not None:\n",
        "        torch.save(best_weights, best_weights_path)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'memory_usage': memory_usage,\n",
        "        'results': results,\n",
        "        'start_time': start_time,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_with_no_improvement': epochs_with_no_improvement,\n",
        "        'best_weights_path': best_weights_path\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pt')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"✅ Checkpoint saved: {checkpoint_path}\")\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    \"\"\"Load complete training state\"\"\"\n",
        "    print(f\"📂 Loading checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def find_latest_checkpoint():\n",
        "    \"\"\"Find the most recent checkpoint\"\"\"\n",
        "    checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('checkpoint_epoch_')]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "\n",
        "    # Sort by epoch number\n",
        "    checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    latest = os.path.join(CHECKPOINT_DIR, checkpoints[-1])\n",
        "    print(f\"🔍 Found latest checkpoint: {latest}\")\n",
        "    return latest\n",
        "\n",
        "# =============================================================================\n",
        "# GPU PERFORMANCE CHECK\n",
        "# =============================================================================\n",
        "def quick_gpu_benchmark():\n",
        "    \"\"\"Quick GPU performance test\"\"\"\n",
        "    print(\"🧪 Testing GPU performance...\")\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"❌ CUDA not available!\")\n",
        "        return False\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    a = torch.randn(2000, 2000, device=device)\n",
        "    b = torch.randn(2000, 2000, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(50):\n",
        "        c = torch.mm(a, b)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    benchmark_time = time.time() - start_time\n",
        "\n",
        "    print(f\"GPU benchmark: {benchmark_time:.2f}s\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "    # Clean up\n",
        "    del a, b, c\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if benchmark_time > 3.0:  # Should be ~1-2s on healthy A100\n",
        "        print(\"⚠️  WARNING: GPU performance seems degraded!\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"✅ GPU performance looks good!\")\n",
        "        return True\n",
        "\n",
        "# =============================================================================\n",
        "# ENHANCED TRAINING FUNCTIONS\n",
        "# =============================================================================\n",
        "def make_train_step_with_monitoring(model):\n",
        "    def train_step(batch):\n",
        "        model.train()\n",
        "        batch_start = time.time()\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        if batch_time > 10.0:  # Warn if batch takes >10s\n",
        "            print(f\"⚠️  Slow batch: {batch_time:.2f}s\")\n",
        "\n",
        "        return loss.item()\n",
        "    return train_step\n",
        "\n",
        "def make_val_step_with_monitoring(model):\n",
        "    def val_step(batch):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            return loss.item()\n",
        "    return val_step\n",
        "\n",
        "def train_chunk(start_epoch, end_epoch, model, train_loader, val_loader, optimizer,\n",
        "                scheduler, train_losses, val_losses, memory_usage, best_val_loss,\n",
        "                epochs_with_no_improvement, start_time):\n",
        "    \"\"\"Train for a chunk of epochs with monitoring\"\"\"\n",
        "\n",
        "    train_step = make_train_step_with_monitoring(model)\n",
        "    val_step = make_val_step_with_monitoring(model)\n",
        "\n",
        "    print(f\"🚀 Training epochs {start_epoch+1} to {end_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        epoch_start = time.time()\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{TOTAL_EPOCHS} ---\")\n",
        "\n",
        "        # Training\n",
        "        train_loss = 0.0\n",
        "        train_samples = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            if batch_idx == 0:  # Only process first batch for testing\n",
        "                loss = train_step(batch)\n",
        "                batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "                train_loss += loss * batch_size_actual\n",
        "                train_samples += batch_size_actual\n",
        "                batch_count += 1\n",
        "                print(f\"  Batch {batch_idx+1} completed, loss: {loss:.6f}\")\n",
        "                break\n",
        "\n",
        "        avg_train_loss = train_loss / train_samples if train_samples > 0 else 0\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0.0\n",
        "        val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                loss = val_step(batch)\n",
        "                batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "                val_loss += loss * batch_size_actual\n",
        "                val_samples += batch_size_actual\n",
        "                if batch_idx >= 4:  # Limit validation batches for speed\n",
        "                    break\n",
        "\n",
        "        avg_val_loss = val_loss / val_samples if val_samples > 0 else 0\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Memory tracking\n",
        "        torch.cuda.synchronize()\n",
        "        memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "        memory_usage.append(memory_mb)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(f\"  ✅ Epoch {epoch+1} completed in {epoch_time:.1f}s\")\n",
        "        print(f\"     Train Loss: {avg_train_loss:.6f}\")\n",
        "        print(f\"     Val Loss: {avg_val_loss:.6f}\")\n",
        "        print(f\"     Memory: {memory_mb:.0f} MB\")\n",
        "        print(f\"     Total Time: {total_time:.1f}s\")\n",
        "\n",
        "        # Scheduler and early stopping\n",
        "        scheduler.step(avg_val_loss)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_weights = model.state_dict()  # Store best weights\n",
        "            torch.save(best_weights, os.path.join(CHECKPOINT_DIR, \"best_weights.pt\"))\n",
        "            epochs_with_no_improvement = 0\n",
        "            print(f\"     🎯 New best validation loss: {best_val_loss:.6f}\")\n",
        "        else:\n",
        "            epochs_with_no_improvement += 1\n",
        "            best_weights = None  # No improvement this epoch\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        save_checkpoint(model, optimizer, scheduler, epoch + 1, train_losses,\n",
        "                       val_losses, memory_usage, [], start_time, best_val_loss,\n",
        "                       epochs_with_no_improvement, best_weights)\n",
        "\n",
        "    return train_losses, val_losses, memory_usage, best_val_loss, epochs_with_no_improvement\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN TRAINING ORCHESTRATOR\n",
        "# =============================================================================\n",
        "def run_resource_managed_training():\n",
        "    \"\"\"Main training function with resource management\"\"\"\n",
        "\n",
        "    # Check GPU performance first\n",
        "    if not quick_gpu_benchmark():\n",
        "        print(\"🔄 Consider restarting runtime for better GPU performance\")\n",
        "        return\n",
        "\n",
        "    # Initialize training parameters\n",
        "    batch_size = 64\n",
        "    learning_rate = 1e-5\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Check for existing checkpoint\n",
        "    latest_checkpoint = find_latest_checkpoint()\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        print(\"🔄 Resuming from checkpoint...\")\n",
        "        # Load model architecture first\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "        model.to(device)\n",
        "\n",
        "        # Initialize optimizer and scheduler\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = load_checkpoint(latest_checkpoint, model, optimizer, scheduler)\n",
        "\n",
        "        # Restore training state\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        val_losses = checkpoint['val_losses']\n",
        "        memory_usage = checkpoint['memory_usage']\n",
        "        start_time = checkpoint['start_time']\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        epochs_with_no_improvement = checkpoint['epochs_with_no_improvement']\n",
        "\n",
        "        print(f\"📍 Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    else:\n",
        "        print(\"🆕 Starting fresh training...\")\n",
        "        # Initialize everything from scratch\n",
        "        start_time = time.time()\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "        model.to(device)\n",
        "\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "        start_epoch = 0\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        memory_usage = []\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_with_no_improvement = 0\n",
        "\n",
        "    print('='*80)\n",
        "    print(f'🎯 Training Configuration:')\n",
        "    print(f'   Batch Size: {batch_size}')\n",
        "    print(f'   Learning Rate: {learning_rate}')\n",
        "    print(f'   Total Epochs: {TOTAL_EPOCHS}')\n",
        "    print(f'   Epochs per Chunk: {EPOCHS_PER_CHUNK}')\n",
        "    print(f'   Trainable Parameters: {model.num_parameters():,}')\n",
        "    print('='*80)\n",
        "\n",
        "    # DEFINE YOUR DATALOADERS HERE\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Calculate remaining epochs and chunks\n",
        "    remaining_epochs = TOTAL_EPOCHS - start_epoch\n",
        "    if remaining_epochs <= 0:\n",
        "        print(\"✅ Training already completed!\")\n",
        "        return\n",
        "\n",
        "    # Determine chunk size for this session\n",
        "    epochs_this_session = min(EPOCHS_PER_CHUNK, remaining_epochs)\n",
        "    end_epoch = start_epoch + epochs_this_session\n",
        "\n",
        "    print(f\"📊 This session: epochs {start_epoch+1} to {end_epoch}\")\n",
        "    print(f\"💾 Results will be saved to: {RESULTS_DIR}\")\n",
        "\n",
        "    # Train the chunk\n",
        "    try:\n",
        "        train_losses, val_losses, memory_usage, best_val_loss, epochs_with_no_improvement = train_chunk(\n",
        "            start_epoch, end_epoch, model, train_loader, val_loader, optimizer,\n",
        "            scheduler, train_losses, val_losses, memory_usage, best_val_loss,\n",
        "            epochs_with_no_improvement, start_time\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Chunk completed successfully!\")\n",
        "        print(f\"   Epochs completed: {end_epoch}/{TOTAL_EPOCHS}\")\n",
        "\n",
        "        # If training complete, save final results\n",
        "        if end_epoch >= TOTAL_EPOCHS:\n",
        "            print(\"🎉 Training completed! Saving final results...\")\n",
        "\n",
        "            # Load best weights\n",
        "            best_weights_path = os.path.join(CHECKPOINT_DIR, \"best_weights.pt\")\n",
        "            if os.path.exists(best_weights_path):\n",
        "                best_weights = torch.load(best_weights_path)\n",
        "                print(f\"✅ Best weights loaded from {best_weights_path}\")\n",
        "            else:\n",
        "                best_weights = model.state_dict()\n",
        "                print(\"⚠️  No best weights file found, using final model state\")\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            results = [{\n",
        "                \"experiment\": \"fft\",\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_losses\": val_losses,\n",
        "                \"memory_usage_MB\": memory_usage,\n",
        "                \"training_time_seconds\": training_time,\n",
        "                \"best_val_loss\": best_val_loss,\n",
        "                \"best_weights\": best_weights  # Added missing best_weights\n",
        "            }]\n",
        "\n",
        "            # Save results\n",
        "            results_df = pd.DataFrame(results)\n",
        "            results_path = os.path.join(RESULTS_DIR, 'ffttrain_target_modules_ablation_results.csv')\n",
        "            results_df.to_csv(results_path, index=False)\n",
        "\n",
        "            print(f\"💾 Final results saved to: {results_path}\")\n",
        "            print(f\"⏱️  Total training time: {training_time:.1f} seconds\")\n",
        "\n",
        "        else:\n",
        "            print(f\"\\n⏭️  Next steps:\")\n",
        "            print(f\"   1. Runtime → Disconnect and delete runtime\")\n",
        "            print(f\"   2. Wait 2-3 minutes\")\n",
        "            print(f\"   3. Reconnect and run this code again\")\n",
        "            print(f\"   4. Training will resume from epoch {end_epoch+1}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during training: {e}\")\n",
        "        print(\"💾 Checkpoint should be saved. You can resume later.\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"🧹 Memory cleaned up\")\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE INSTRUCTIONS\n",
        "# =============================================================================\n",
        "print(\"\"\"\n",
        "🚀 RESOURCE-MANAGED TRAINING SETUP\n",
        "=====================================\n",
        "\n",
        "BEFORE RUNNING:\n",
        "1. Define your train_loader and val_loader\n",
        "2. Ensure your dataset and collate_fn are ready\n",
        "3. Update CHECKPOINT_DIR and RESULTS_DIR paths if needed\n",
        "\n",
        "TO RUN:\n",
        "run_resource_managed_training()\n",
        "\n",
        "WORKFLOW:\n",
        "- Trains 4 epochs per session\n",
        "- Saves checkpoints automatically\n",
        "- Disconnects → waits → reconnects for fresh GPU\n",
        "- Resumes automatically from latest checkpoint\n",
        "- Saves final results when complete\n",
        "\n",
        "BENEFITS:\n",
        "✅ Avoids degraded GPU instances\n",
        "✅ Saves compute units with efficient training\n",
        "✅ Automatic resume from failures\n",
        "✅ Memory management built-in\n",
        "✅ Performance monitoring\n",
        "\"\"\")\n",
        "\n",
        "# Uncomment to run (after adding your dataloaders):\n",
        "run_resource_managed_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpuhqNKXDnIZ"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MA4ee1IC6wu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f0m7_YrDkUT"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = '/content/drive/MyDrive/GIT_checkpoints/'\n",
        "# Fix 1: Set tokenizer padding to left side for decoder-only models\n",
        "processor.tokenizer.padding_side = 'left'\n",
        "best_weights_path = os.path.join(CHECKPOINT_DIR, \"best_weights.pt\")\n",
        "# Load and prepare model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n",
        "model.load_state_dict(torch.load('best_weights.pt', weights_only=True))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loss = 0.0\n",
        "test_samples = 0\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "results = []\n",
        "bleu = load(\"bleu\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "        test_loss += loss.item() * batch_size_actual\n",
        "        test_samples += batch_size_actual\n",
        "\n",
        "        # Generate predictions\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "\n",
        "        # Fix 2: Use max_new_tokens instead of max_length to avoid length conflicts\n",
        "        generated_ids = model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=92,  # Generate up to 50 new tokens\n",
        "            do_sample=False,    # Use greedy decoding for reproducible results\n",
        "            pad_token_id=processor.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        predictions.extend([text.replace(\"List pathalogical findings for this chest X-ray:\", \"\").strip() for text in generated_texts])\n",
        "\n",
        "        # Decode labels for references\n",
        "        label_ids = batch[\"labels\"]\n",
        "        reference_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        references.extend(reference_texts)\n",
        "        break\n",
        "\n",
        "avg_test_loss = test_loss / test_samples\n",
        "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "print(f\"Test Results with Best Model:\")\n",
        "print(f\"Test Loss: {avg_test_loss:.6f}, BLEU Score: {bleu_score['bleu']:.4f}\")\n",
        "results.append({\n",
        "        \"experiment\": \"fftTesting\",\n",
        "        \"test_loss\": test_loss,\n",
        "        \"bleu_score\": bleu_score\n",
        "    })\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "folder_path = '/content/drive/MyDrive/GIT AblationStratergy/fft_test_ablationStudy/results'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(os.path.join(folder_path, 'fftTest_target_modules_ablation_results.csv'), index=False)\n",
        "print(\"\\nFFT Ablation Study Testing Results (Target Modules):\")\n",
        "print(results_df[[\"experiment\", \"test_loss\", \"bleu_score\", \"training_time_seconds\", \"trainable_parameters\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImxUv-P-GfDX",
        "outputId": "126c8757-ae84-4fd6-9dd0-931e3f63dc81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXl_6NBw4Wla",
        "outputId": "09003937-94dc-4233-b775-d50a5cbdbea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory Allocated: 2.8GB\n",
            "GPU Memory Reserved: 33.3GB\n",
            "GPU Utilization: 0, 54.05\n"
          ]
        }
      ],
      "source": [
        "# Check what's actually running\n",
        "import torch\n",
        "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1e9:.1f}GB\")\n",
        "print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1e9:.1f}GB\")\n",
        "\n",
        "# See GPU utilization\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,power.draw', '--format=csv,noheader,nounits'], capture_output=True, text=True)\n",
        "print(f\"GPU Utilization: {result.stdout.strip()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
